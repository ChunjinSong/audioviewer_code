# Config for training SoftIntroVAE on CelebA-HQ at resolution 256x256

NAME: celeba-hq256
DATASET:
  PART_COUNT: 16
  SIZE: 29000
  SIZE_TEST: 1000
  PATH: ../dataset/celebhq_256_tfrecords/celeba-r%02d.tfrecords.%03d
  PATH_TEST: ../dataset/celebhq_256_test_tfrecords/celeba-r%02d.tfrecords.%03d
  MAX_RESOLUTION_LEVEL: 8
  SAMPLES_PATH: ../dataset/test_recon_celebhq

MODEL:
  LATENT_SPACE_SIZE:  512 # origin-512 scj-128
  LAYER_COUNT: 7
  MAX_CHANNEL_COUNT: 512 # origin-512  scj-128
  START_CHANNEL_COUNT: 64
  DLATENT_AVG_BETA: 0.995
  MAPPING_LAYERS: 8
  BETA_KL: 0.1
  BETA_REC: 0.05
  BETA_NEG: [2048,  2048,   2048,   1024,  512,   512,     512,    512,    512]
  SCALE: 0.000005

OUTPUT_DIR: ../output/model/celeba-hq256/
TRAIN:
  BASE_LEARNING_RATE: 0.002
  EPOCHS_PER_LOD: 30
  NUM_VAE: 1
  LEARNING_DECAY_RATE: 0.1
  LEARNING_DECAY_STEPS: []
  TRAIN_EPOCHS: 300
  #                    4       8       16       32       64       128        256       512       1024
  LOD_2_BATCH_8GPU: [512,    256,     128,      64,      32,       32,        32,       32,        24]
  LOD_2_BATCH_4GPU: [512,    256,     128,      64,      32,       32,        16,       8,        4]
  LOD_2_BATCH_2GPU: [128,    128,     128,      64,      32,       16,        8]
  LOD_2_BATCH_1GPU: [128,    128,     128,      32,      16,       8,         4]

  LEARNING_RATES: [0.0015,  0.0015,   0.0015,   0.0015,  0.0015,   0.0015,     0.0015,     0.003,    0.003]
